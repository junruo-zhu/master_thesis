{"cells":[{"cell_type":"markdown","metadata":{"id":"Fo-Oig4Yib5K"},"source":["# BERTopic years' trend\n","\n","This script aims to generate latent topics in each year from input data sets.\n","\n","Input file: csv files for the sentiment and emotion analysis.\n","Output file:\n","\n","1.   topic infomation in each year including topic representative documents and keywords\n","2.   corresponding models\n","3.   the distribution probabilities of each topic for every text\n","4.   the distribution probabilities of each topic every year\n","\n","\n","\n","**NOTE:**\n","The script is adapted from https://colab.research.google.com/drive/1BoQ_vakEVtojsd2x_U6-_x52OOuqruj2?usp=sharing#scrollTo=Fo-Oig4Yib5K\n","\n","In acknowledgment of the contributions made, portions of this code were developed with the guidance and assistance of ChatGPT.\n"]},{"cell_type":"markdown","metadata":{"id":"wPP4HfpJixEK"},"source":["# Enabling the GPU\n","\n","First, enable GPUs for the notebook:\n","\n","- Navigate to Editâ†’Notebook Settings\n","- select GPU from the Hardware Accelerator drop-down\n","\n","[Reference](https://colab.research.google.com/notebooks/gpu.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"k5RBp4gFuSaR"},"source":["# **Installing BERTopic**\n","\n","We start by installing BERTopic from PyPi:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mNWv-K-xiZsA"},"outputs":[],"source":["%%capture\n","!pip install bertopic\n","!pip install sentence-transformers\n","!pip install umap-learn\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"1Qdqzq2muUm5"},"source":["## Restart the Notebook\n","After installing BERTopic, some packages that were already loaded were updated and in order to correctly use them, we should now restart the notebook.\n","\n","From the Menu:\n","\n","Runtime â†’ Restart Runtime"]},{"cell_type":"markdown","metadata":{"id":"48mcztmmJArk"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"2vCrsZ1XuZTF"},"source":["## **Data**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TE67L83cuLzs"},"outputs":[],"source":["# Parameters\n","corpus = \"red\" # ðŸŸ¡Only change this one\n","min_cluster_size = 350 # ðŸŸ¡Only change this one\n","size = str(min_cluster_size)\n","\n","# input data\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","input_path = f\"/content/drive/MyDrive/Colab Notebooks/Masters_Thesis/0_corpus/preprocessed_for_sentiment_analysis/{corpus}_sentiment_df.csv\"\n","dataset = pd.read_csv(input_path)\n","\n","# output paths\n","output_csv_path = f\"/content/drive/MyDrive/Colab Notebooks/Masters_Thesis/5_results/{corpus}_{size}_yearly_trend_topic_info.csv\"\n","output_model_path = f\"/content/drive/MyDrive/Colab Notebooks/Masters_Thesis/5_results/\"\n","output_full_csv_path = f\"/content/drive/MyDrive/Colab Notebooks/Masters_Thesis/5_results/{corpus}_{size}_text_topic_label_prob.csv\"\n","output_yearly_trend_per_topic_path = f\"/content/drive/MyDrive/Colab Notebooks/Masters_Thesis/5_results/{corpus}_{size}_yearly_trend_per_topic.csv\"\n","\n","\n","# Extract abstracts to train on and corresponding titles\n","abstracts = dataset[\"text\"]\n","abstracts = abstracts.fillna(\"\")\n","abstracts = abstracts.astype(str)\n","\n","# # Load the model, if you have already got a trained model\n","# from sentence_transformers import SentenceTransformer\n","# from bertopic import BERTopic\n","\n","# path = \"/content/drive/MyDrive/Colab Notebooks/Masters_Thesis/5_results/model\"\n","# # Define embedding model\n","# embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","\n","# # Load model and add embedding model\n","# topic_model = BERTopic.load(path, embedding_model=embedding_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R_BoAGzxwNHd"},"outputs":[],"source":["abstracts[0]"]},{"cell_type":"markdown","metadata":{"id":"sVfnYtUaxyLT"},"source":["## **Pre-calculate Embeddings**\n","After having created our data, namely `abstracts`, we can dive into the very first best practice, **pre-calculating embeddings**.\n","\n","BERTopic works by converting documents into numerical values, called embeddings. This process can be very costly, especially if we want to iterate over parameters. Instead, we can calculate those embeddings once and feed them to BERTopic to skip calculating embeddings each time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTwXxSnPupbZ"},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","\n","# Pre-calculate embeddings\n","embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","embeddings = embedding_model.encode(abstracts, show_progress_bar=True)"]},{"cell_type":"markdown","metadata":{"id":"28_EVoOfyZLb"},"source":["## **Preventing Stochastic Behavior**\n","In BERTopic, we generally use a dimensionality reduction algorithm to reduce the size of the embeddings. This is done to prevent the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) to a certain degree.\n","\n","As a default, this is done with [UMAP](https://github.com/lmcinnes/umap) which is an incredible algorithm for reducing dimensional space. However, by default, it shows stochastic behavior which creates different results each time you run it. To prevent that, we will need to set a `random_state` of the model before passing it to BERTopic.\n","\n","As a result, we can now fully reproduce the results each time we run the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbI9VptGxop4"},"outputs":[],"source":["from umap import UMAP\n","\n","umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric=\"cosine\", random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"TH6vZPGU2zpg"},"source":["## **Controlling Number of Topics**\n","There is a parameter to control the number of topics, namely `nr_topics`. This parameter, however, merges topics **after** they have been created. It is a parameter that supports creating a fixed number of topics.\n","\n","However, it is advised to control the number of topics through the cluster model which is by default HDBSCAN. HDBSCAN has a parameter, namely `min_topic_size` that indirectly controls the number of topics that will be created.\n","\n","A higher `min_topic_size` will generate fewer topics and a lower `min_topic_size` will generate more topics.\n","\n","Here, we will go with `min_topic_size=40` to get around XXX topics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Swmkcx5S3e9m"},"outputs":[],"source":["from hdbscan import HDBSCAN\n","\n","hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, metric=\"euclidean\", cluster_selection_method=\"eom\", prediction_data=True)"]},{"cell_type":"markdown","metadata":{"id":"66zgeCyf0jy3"},"source":["## **Improving Default Representation**\n","The default representation of topics is calculated through [c-TF-IDF](https://maartengr.github.io/BERTopic/algorithm/algorithm.html#5-topic-representation). However, c-TF-IDF is powered by the [CountVectorizer](https://maartengr.github.io/BERTopic/getting_started/vectorizers/vectorizers.html) which converts text into tokens. Using the CountVectorizer, we can do a number of things:\n","\n","* Remove stopwords\n","* Ignore infrequent words\n","* Increase\n","\n","In other words, we can preprocess the topic representations **after** documents are assigned to topics. This will not influence the clustering process in any way.\n","\n","Here, we will ignore English stopwords and infrequent words. Moreover, by increasing the n-gram range we will consider topic representations that are made up of one or two words."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xyIFi06Vzeg3"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction import text\n","\n","stop_words = text.ENGLISH_STOP_WORDS.union([\"ve\", \"ha\",\"don\",\"did\",\"ll\",\n","                                            \"climate\", \"change\", \"just\",\n","                                            \"like\",\"think\",\"really\",\"going\",\n","                                            \"thank\",\"thanks\",\"weclome\",\n","                                            \"lol\", \"ok\", \"okay\",\"lmao\",\n","                                            \"sorry\",\"sure\",\"isn\",'yes',\n","                                            'oh', 'yeah', 'shit', 'duh',\n","                                            'fuck', 'checks', 'boe', 'huh',\n","                                            'people'])\n","stop_words = list(stop_words)\n","\n","vectorizer_model = CountVectorizer(stop_words=stop_words, min_df=0.01, ngram_range=(1, 2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TCCmIs1gn2YT"},"outputs":[],"source":["from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n","print(ENGLISH_STOP_WORDS)"]},{"cell_type":"markdown","metadata":{"id":"cIu9afMo1YYg"},"source":["## **Additional Representations**\n","Previously, we have tuned the default representation but there are quite a number of [other topic representations](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html) in BERTopic that we can choose from. From [KeyBERTInspired](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired) and [PartOfSpeech](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#partofspeech), to [OpenAI\"s ChatGPT](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#chatgpt) and [open-source](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#langchain) alternatives, many representations are possible.\n","\n","In BERTopic, you can model many different topic representations simultanously to test them out and get different perspectives of topic descriptions. This is called [multi-aspect](https://maartengr.github.io/BERTopic/getting_started/multiaspect/multiaspect.html) topic modeling.\n","\n","Here, we will demonstrate a number of interesting and useful representations in BERTopic:\n","\n","* KeyBERTInspired\n","  * A method that derives inspiration from how KeyBERT works\n","* PartOfSpeech\n","  * Using SpaCy\"s POS tagging to extract words\n","* MaximalMarginalRelevance\n","  * Diversify the topic words\n","* OpenAI\n","  * Use ChatGPT to label our topics\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uaxb00nfzejc"},"outputs":[],"source":["from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\n","\n","# KeyBERT\n","keybert_model = KeyBERTInspired()\n","\n","# Part-of-Speech\n","pos_model = PartOfSpeech(\"en_core_web_sm\")\n","\n","# MMR\n","mmr_model = MaximalMarginalRelevance(diversity=0.3)\n","\n","# All representation models\n","representation_model = {\n","    \"KeyBERT\": keybert_model,\n","    \"MMR\": mmr_model,\n","    \"POS\": pos_model\n","}"]},{"cell_type":"markdown","metadata":{"id":"1yjoDpUxDItK"},"source":["## **Training**\n","Now that we have a set of best practices, we can use them in our training loop. Here, several different representations, keywords and labels for our topics will be created. If you want to iterate over the topic model it is advised to use the pre-calculated embeddings as that significantly speeds up training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3K4ehYozTBZ"},"outputs":[],"source":["from bertopic import BERTopic\n","\n","# # If the results do not make sense, then change this parameter and run it again.\n","# min_cluster_size = 350\n","# size = str(min_cluster_size)\n","# hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, metric=\"euclidean\", cluster_selection_method=\"eom\", prediction_data=True)\n","# output_csv_path = f\"/content/drive/MyDrive/Colab Notebooks/Masters_Thesis/5_results/{corpus}_{size}_yearly_trend_topic_info.csv\"\n","# output_model_path = f\"/content/drive/MyDrive/Colab Notebooks/Masters_Thesis/5_results/\"\n","# output_full_csv_path = f\"/content/drive/MyDrive/Colab Notebooks/Masters_Thesis/5_results/{corpus}_{size}_text_topic_label_prob.csv\"\n","# output_yearly_trend_per_topic_path = f\"/content/drive/MyDrive/Colab Notebooks/Masters_Thesis/5_results/{corpus}_{size}_yearly_trend_per_topic.csv\"\n","\n","topic_model = BERTopic(\n","\n","  # Pipeline models\n","  embedding_model=embedding_model,\n","  umap_model=umap_model,\n","  hdbscan_model=hdbscan_model,\n","  vectorizer_model=vectorizer_model,\n","  representation_model=representation_model,\n","\n","  # Hyperparameters\n","  top_n_words=10,\n","  verbose=True,\n","  calculate_probabilities=True # show probs of all topics for each text\n",")\n","\n","topics, probs = topic_model.fit_transform(abstracts, embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3lwA8m7u3uLC"},"outputs":[],"source":["topic_model.get_topic_info()"]},{"cell_type":"markdown","metadata":{"id":"ueQylo5D3ziw"},"source":["Save the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w3WRXoRP2ej8"},"outputs":[],"source":["topic_info = topic_model.get_topic_info()\n","topic_info_df = pd.DataFrame(topic_info)\n","topic_info_df.to_csv(output_csv_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"hUYo2tokvhCL"},"source":["To get all representations for a single topic, we simply run the following:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CmfKtFIcvkrx"},"outputs":[],"source":["topic_model.get_topic(0, full=True)"]},{"cell_type":"markdown","metadata":{"id":"fZeQ32r8CeNi"},"source":["**NOTE**: The labels generated by OpenAI\"s **ChatGPT** are especially interesting to use throughout your model. Below, we will go into more detail how to set that as a custom label."]},{"cell_type":"markdown","metadata":{"id":"dVPbgnGKJJYk"},"source":["# Presenting"]},{"cell_type":"markdown","metadata":{"id":"DjMgbsVwx0WA"},"source":["## **(Custom) Labels**\n","The default label of each topic are the top 3 words in each topic combined with an underscore between them.\n","\n","This, of course, might not be the best label that you can think of for a certain topic. Instead, we can use `.set_topic_labels` to manually label all or certain topics.\n","\n","We can also use `.set_topic_labels` to use one of the other topic representations that we had before, like `KeyBERTInspired` or even `OpenAI`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11967,"status":"ok","timestamp":1707495038326,"user":{"displayName":"zhu junruo","userId":"03614061717994347905"},"user_tz":-60},"id":"Eqkw8GKnx896","outputId":"656fbea5-9d1e-4307-e9a1-1e67e688f673"},"outputs":[{"name":"stdout","output_type":"stream","text":["        id  year                                               text  topic  \\\n","0  c7w2a9f  2013  Discussing climate change with a skeptic on an...     -1   \n","1  c7x3p76  2013  That hasn't even been considered for several y...     -1   \n","2  c7xjxtf  2013  anything on non- carbon dioxide GHGs? I though...      1   \n","3  c7xkqi8  2013  That would be easy to find as well since there...     -1   \n","4  c7xp7wy  2013                                       Cool, thanks      0   \n","\n","                                               probs  \n","0  [0.008548205611880074, 0.007631884661650595, 0...  \n","1  [0.0023238444123834063, 0.0047248330141559675,...  \n","2  [0.009350049445619481, 0.10805928031131751, 0....  \n","3  [0.0024232135283891506, 0.005028369221534361, ...  \n","4  [0.6955159078876341, 0.00818598263243194, 0.01...  \n"]}],"source":["# label dictionary\n","topic_to_label = {\n","\n","}\n","\n","# Label the topics yourself\n","topic_model.set_topic_labels(topic_to_label)\n","\n","# Update the dataset and map the labels to topics\n","dataset[\"topic\"] = topics\n","dataset[\"probs\"] = probs.tolist()\n","# dataset[\"label\"] = dataset[\"topic\"].map(topic_to_label)\n","\n","# Check the results\n","print(dataset.head())\n","\n","# Save the csv\n","dataset.to_csv(output_full_csv_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"gPk6VfFh2hOR"},"source":["**ðŸ”¥ Tip - Parameters ðŸ”¥**\n","***\n","If you would like to return the topic-document probability matrix, then it is advised to use `calculate_probabilities=True`. Do note that this can significantly slow down training. To speed it up, use [cuML\"s HDBSCAN](https://maartengr.github.io/BERTopic/getting_started/clustering/clustering.html#cuml-hdbscan) instead. You could also approximate the topic-document probability matrix with `.approximate_distribution` which will be discussed later.\n","***"]},{"cell_type":"markdown","metadata":{"id":"XnGQDTFmewPH"},"source":["## Topical Trend Anslysis"]},{"cell_type":"markdown","metadata":{"id":"qcaJgjIDxmQy"},"source":["### Calculate yearly average probs per topic and define functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDvBpcHAe1TJ"},"outputs":[],"source":["# creat a hash to save the topic weight distribution of each text and the total num of texts\n","# if a text is assigned to the Topic -1 (Noise), I still count its disctibution (vector)\n","yearly_topic_probs_sum = {}\n","yearly_texts_count = {}\n","\n","for index, row in dataset.iterrows():\n","    year = row[\"year\"]\n","    topic_probs = row[\"probs\"]\n","\n","    # make sure that every year's topic weight disctribution is saved\n","    if year not in yearly_topic_probs_sum:\n","        yearly_topic_probs_sum[year] = [0] * len(topic_probs)\n","        yearly_texts_count[year] = 0\n","\n","    # sum up the prob of each topic respectively\n","    yearly_topic_probs_sum[year] = [sum(x) for x in zip(yearly_topic_probs_sum[year], topic_probs)]\n","    # save the number of texts of each year\n","    yearly_texts_count[year] += 1\n","\n","# calculate the avg prob of each topic in every year\n","yearly_avg_topic_probs = {year: [prob / yearly_texts_count[year] for prob in probs]\n","                          for year, probs in yearly_topic_probs_sum.items()}\n","\n","\n","# Save the yearly average topic probs\n","label_probs_df = pd.DataFrame(list(yearly_avg_topic_probs.items()), columns=[\"year\", \"probs\"])\n","label_probs_df.to_csv(output_yearly_trend_per_topic_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"VVk0FPLu7v2r"},"source":["## **Serialization**\n","\n","When saving a BERTopic model, there are several ways in doing so. You can either save the entire model with `pickle`, `pytorch`, or `safetensors`.\n","\n","Personally, I would advise going with `safetensors` whenever possible. The reason for this is that the format allows for a very small topic model to be saved and shared.\n","\n","When saving a model with `safetensors`, it skips over saving the dimensionality reduction and clustering models. The `.transform` function will still work without these models but instead assign topics based on the similarity between document embeddings and the topic embeddings.\n","\n","As a result, the `.transform` step might give different results but it is generally worth it considering the smaller and significantly faster model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAzSRDQxLdM5"},"outputs":[],"source":["# Save the model\n","embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n","topic_model.save(output_model_path, serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1v1Oycb6aG_cczoRMIVpDBgIFJP2a6jjl","timestamp":1705341291480},{"file_id":"12dQ0NqVkmMiNrjFkLQUj5QDSXShq6IQq","timestamp":1704382539465},{"file_id":"1BoQ_vakEVtojsd2x_U6-_x52OOuqruj2","timestamp":1704305885117}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
